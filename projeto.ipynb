{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dependências do Projeto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in e:\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: trio~=0.17 in e:\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in e:\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in e:\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in e:\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: outcome in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in e:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in e:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in e:\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in e:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in e:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in e:\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in e:\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in e:\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "#Instalando o pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in e:\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogui in e:\\anaconda3\\lib\\site-packages (0.9.53)\n",
      "Requirement already satisfied: PyTweening>=1.0.1 in e:\\anaconda3\\lib\\site-packages (from pyautogui) (1.0.4)\n",
      "Requirement already satisfied: mouseinfo in e:\\anaconda3\\lib\\site-packages (from pyautogui) (0.1.3)\n",
      "Requirement already satisfied: pymsgbox in e:\\anaconda3\\lib\\site-packages (from pyautogui) (1.0.9)\n",
      "Requirement already satisfied: pygetwindow>=0.0.5 in e:\\anaconda3\\lib\\site-packages (from pyautogui) (0.0.9)\n",
      "Requirement already satisfied: pyscreeze>=0.1.21 in e:\\anaconda3\\lib\\site-packages (from pyautogui) (0.1.28)\n",
      "Requirement already satisfied: pyrect in e:\\anaconda3\\lib\\site-packages (from pygetwindow>=0.0.5->pyautogui) (0.2.0)\n",
      "Requirement already satisfied: pyperclip in e:\\anaconda3\\lib\\site-packages (from mouseinfo->pyautogui) (1.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#Caso não consiga pydirectinput\n",
    "%pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in e:\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: protobuf==3.20.* in e:\\anaconda3\\lib\\site-packages (3.20.3)\n",
      "Requirement already satisfied: cloudpickle in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.5.2)\n",
      "Requirement already satisfied: pandas in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: torch>=1.11 in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata~=4.13 in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.13.0)\n",
      "Requirement already satisfied: gym==0.21 in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.21.5)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.12.0)\n",
      "Requirement already satisfied: psutil in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: pillow in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (9.2.0)\n",
      "Requirement already satisfied: opencv-python in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.7.0.68)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: ale-py==0.7.4 in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.7.4)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.64.1)\n",
      "Requirement already satisfied: rich in e:\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.3.1)\n",
      "Requirement already satisfied: importlib-resources in e:\\anaconda3\\lib\\site-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.10.2)\n",
      "Requirement already satisfied: click in e:\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.28.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in e:\\anaconda3\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.5.4)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\anaconda3\\lib\\site-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.8.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.16.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.51.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (63.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in e:\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\lib\\site-packages (from torch>=1.11->stable-baselines3[extra]) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.14.0 in e:\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in e:\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in e:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in e:\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.1.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: libtorrent in e:\\anaconda3\\lib\\site-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in e:\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#Biblioteca de aprendizado por reforço\n",
    "%pip install stable-baselines3[extra] protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mss in e:\\anaconda3\\lib\\site-packages (7.0.1)\n",
      "Requirement already satisfied: pydirectinput in e:\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: pytesseract in e:\\anaconda3\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging>=21.3 in e:\\anaconda3\\lib\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in e:\\anaconda3\\lib\\site-packages (from pytesseract) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (e:\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install mss pydirectinput pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para captura de tela\n",
    "from mss import mss\n",
    "#import pyautogui #para comandos de teclado\n",
    "import cv2 \n",
    "#Tratamentos dos frames\n",
    "import numpy as np \n",
    "import pyautogui\n",
    "#Framework intermediário paara trabalhar com as imagens\n",
    "#import pytesseract #para extrair o game over da imagem\n",
    "from matplotlib import pyplot as plt #Visualizando resultados\n",
    "import time\n",
    "from PIL import Image\n",
    "#Coisas para construir o ambinete\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete, Dict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import base64\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Construindo o Ambiente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe para definir o ambiente\n",
    "class WebGame(Env):\n",
    "    #ESSAS FUNÇÃO SÃO NECESSÁRIAS POR CONTA DO GYM\n",
    "    #Função responsavel pela inicialização do ambiente;\n",
    "    #Portanto onde ficam todas as variaveis do ambiente, configuração das ações e da observação \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Configuração do tempo\n",
    "        self.print_time = .005\n",
    "        self.wait_time = .015\n",
    "        \n",
    "        #Configuração do espaço\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,83,200), dtype=np.uint8)\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        #Variavei para a captura de tela\n",
    "        self.screen_cap = mss() #Lib de captura de tela\n",
    "        self.game_location = {\"top\": 300, \"left\": 0, \"width\": 650, \"height\": 500} #Área do monitor com \n",
    "        self.game_done = {\"top\": 40, \"left\": 0, \"width\": 800, \"height\": 640}\n",
    "\n",
    "        self.chromedriver_path = \"./chromedriver.exe\"\n",
    "        _chrome_options = webdriver.ChromeOptions()\n",
    "        _chrome_options.add_argument(\"--mute-audio\")\n",
    "        _chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self._driver = webdriver.Chrome(executable_path=self.chromedriver_path,options=_chrome_options)\n",
    "\n",
    "        self.last_image = np.zeros((500,650,3))\n",
    "        \n",
    "        #Dicionário de ações\n",
    "        self.action_dict = {\n",
    "            0:Keys.ARROW_UP,\n",
    "            1:'nothing'\n",
    "        }\n",
    "\n",
    "    #Função responsavel por passar as ações pro jogo, para fazer algo pro jogo\n",
    "    def step(self, action):\n",
    "        # Se ação escolhida for um dos botões, realizar a ação.\n",
    "        if action != 1:\n",
    "            self._driver.find_elements(By.TAG_NAME, \"body\")[0].send_keys(self.action_dict[int(action)])\n",
    "        \n",
    "        #Checa a próxima ação\n",
    "        observation = self.get_observation()\n",
    "        #Todas as ações tem que verificar se a ação acabou\n",
    "        done, done_cap = self.get_done() \n",
    "        \n",
    "        #Pegando o score do jogo\n",
    "        score = self.get_score()\n",
    "        #Ganhamos 1 ponto por cada frame que estamos vivos.\n",
    "        reward = 1\n",
    "        #É um dicionário de informações que retornam através do que a gente precisa\n",
    "        info = {\n",
    "            'score':score\n",
    "        }\n",
    "\n",
    "        time.sleep(self.wait_time)\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "        \n",
    "\n",
    "    #Função para a visualização do jogo:\n",
    "    def render(self, mode: str='human'):\n",
    "        img = cv2.cvtColor(self._get_image(), cv2.COLOR_BGR2RGB)\n",
    "        if mode == 'rgb_array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            from gym.envs.classic_control import rendering\n",
    "            if self.viewer is None:\n",
    "                self.viewer = rendering.SimpleImageViewer()\n",
    "            self.viewer.imshow(img)\n",
    "            return self.viewer.isopen\n",
    "\n",
    "        # cv2.imshow('Game', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     self.close()\n",
    "\n",
    "    #Função responsavel por recomeçar o jogo\n",
    "    def reset(self):\n",
    "        #Try catch precisa existir pq toda a vez q o selenium detecta que ele viu o chrome dino,\n",
    "        #ele dispara um erro que esta sem internet.\n",
    "        try:\n",
    "            self._driver.get('chrome://dino')\n",
    "            \n",
    "        except WebDriverException:\n",
    "            pass\n",
    "        WebDriverWait(self._driver, 10).until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.CLASS_NAME, \n",
    "                \"runner-canvas\"\n",
    "            ))\n",
    "        )\n",
    "        self._driver.implicitly_wait(0.5)\n",
    "        self._driver.find_elements(By.TAG_NAME, \"body\")[0].send_keys(Keys.ARROW_UP)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    #FUNÇÕES CUSTOM\n",
    "    #Fecha a parte de visualização\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    \n",
    "    def get_score_shape(self):\n",
    "        shape = np.zeros(1)\n",
    "        shape[0] = int(self.get_score())\n",
    "        return shape\n",
    "    \n",
    "    def get_img(self):\n",
    "        LEADING_TEXT = \"data:image/png;base64,\"\n",
    "        canvas = self._driver.execute_script(\"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
    "        img = canvas[len(LEADING_TEXT):]\n",
    "        img_data = np.array(Image.open(BytesIO(base64.b64decode(img))))\n",
    "        return img_data \n",
    "        # img = self._driver.execute_script()\n",
    "    \n",
    "    #Função responsavel por pegar a imagem do jogo e tratar da forma necessária.\n",
    "    def get_print(self): \n",
    "\n",
    "        #img = numpy.array(self.screen_cap.grab(game_location))[:,:,3].astype(np.uint8)\n",
    "        raw = np.array(self.get_img())[:,:,:3].astype(np.uint8)\n",
    "        img = raw[:200,:400]\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (200,83))\n",
    "        channel = np.reshape(resized, (1, 83,200))\n",
    "        self.last_image = raw\n",
    "        \n",
    "        return channel\n",
    "    \n",
    "    #Função responsavel por pegar a imagem do jogo e tratar da forma necessária.\n",
    "    def get_observation(self): \n",
    "        return self.get_print()\n",
    "\n",
    "\n",
    "    #Função para pegar o texto de fim de jogo:\n",
    "    def get_done(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\"), self.last_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os \n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EventCallback\n",
    "# Verificando se o ambiente é válido para fazer as coisa\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, SubprocVecEnv, VecNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, PPO, DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack,SubprocVecEnv\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.logger import Figure, configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    env = Monitor(WebGame())\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    #env = VecFrameStack(env, 4, channels_order='first')\n",
    "    #env = VecNormalize(env, norm_obs=True, norm_reward=True,\n",
    "    #               clip_obs=10.)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_8856\\1390173097.py:26: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self._driver = webdriver.Chrome(executable_path=self.chromedriver_path,options=_chrome_options)\n"
     ]
    }
   ],
   "source": [
    "#env_lambda = lambda: WebGame()\n",
    "#env2 = SubprocVecEnv([env_lambda for i in range(4)])\n",
    "env2 = get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criandos as pastas para que funcione.\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando o modelo em uma pasta chamado de treino\n",
    "import os\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.logger import Logger\n",
    "\n",
    "from stable_baselines3.common import base_class  # pytype: disable=pyi-error\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, sync_envs_normalization\n",
    "\n",
    "class EvalCallbackButBetter(EventCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: Union[gym.Env, VecEnv],\n",
    "        callback_on_new_best: Optional[BaseCallback] = None,\n",
    "        callback_after_eval: Optional[BaseCallback] = None,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        log_path: Optional[str] = None,\n",
    "        best_model_save_path: Optional[str] = None,\n",
    "        deterministic: bool = True,\n",
    "        render: bool = False,\n",
    "        verbose: int = 1,\n",
    "        warn: bool = True,\n",
    "    ):\n",
    "        super().__init__(callback_after_eval, verbose=verbose)\n",
    "\n",
    "        self.callback_on_new_best = callback_on_new_best\n",
    "        if self.callback_on_new_best is not None:\n",
    "            # Give access to the parent\n",
    "            self.callback_on_new_best.parent = self\n",
    "\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.last_mean_reward = -np.inf\n",
    "        self.deterministic = deterministic\n",
    "        self.render = render\n",
    "        self.warn = warn\n",
    "\n",
    "        # Convert to VecEnv for consistency\n",
    "        if not isinstance(eval_env, VecEnv):\n",
    "            eval_env = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "        self.eval_env = eval_env\n",
    "        self.best_model_save_path = best_model_save_path\n",
    "        # Logs will be written in ``evaluations.npz``\n",
    "        if log_path is not None:\n",
    "            log_path = os.path.join(log_path, \"evaluations\")\n",
    "        self.log_path = log_path\n",
    "        self.evaluations_results = []\n",
    "        self.evaluations_timesteps = []\n",
    "        self.evaluations_length = []\n",
    "        # For computing success rate\n",
    "        self._is_success_buffer = []\n",
    "        self.evaluations_successes = []\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Does not work in some corner cases, where the wrapper is not the same\n",
    "        if not isinstance(self.training_env, type(self.eval_env)):\n",
    "            warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
    "\n",
    "        # Create folders if needed\n",
    "        if self.best_model_save_path is not None:\n",
    "            os.makedirs(self.best_model_save_path, exist_ok=True)\n",
    "        if self.log_path is not None:\n",
    "            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n",
    "\n",
    "        # Init callback called on new best model\n",
    "        if self.callback_on_new_best is not None:\n",
    "            self.callback_on_new_best.init_callback(self.model)\n",
    "\n",
    "    def _log_success_callback(self, locals_: Dict[str, Any], globals_: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Callback passed to the  ``evaluate_policy`` function\n",
    "        in order to log the success rate (when applicable),\n",
    "        for instance when using HER.\n",
    "        :param locals_:\n",
    "        :param globals_:\n",
    "        \"\"\"\n",
    "        info = locals_[\"info\"]\n",
    "\n",
    "        if locals_[\"done\"]:\n",
    "            maybe_is_success = info.get(\"is_success\")\n",
    "            if maybe_is_success is not None:\n",
    "                self._is_success_buffer.append(maybe_is_success)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = True\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Sync training and eval env if there is VecNormalize\n",
    "            if self.model.get_vec_normalize_env() is not None:\n",
    "                try:\n",
    "                    sync_envs_normalization(self.training_env, self.eval_env)\n",
    "                except AttributeError as e:\n",
    "                    raise AssertionError(\n",
    "                        \"Training and eval env are not wrapped the same way, \"\n",
    "                        \"see https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#evalcallback \"\n",
    "                        \"and warning above.\"\n",
    "                    ) from e\n",
    "\n",
    "            # Reset success rate buffer\n",
    "            self._is_success_buffer = []\n",
    "\n",
    "            episode_rewards, episode_lengths = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                render=self.render,\n",
    "                deterministic=self.deterministic,\n",
    "                return_episode_rewards=True,\n",
    "                warn=self.warn,\n",
    "                callback=self._log_success_callback,\n",
    "            )\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)\n",
    "                self.evaluations_length.append(episode_lengths)\n",
    "\n",
    "                kwargs = {}\n",
    "                # Save success log if present\n",
    "                if len(self._is_success_buffer) > 0:\n",
    "                    self.evaluations_successes.append(self._is_success_buffer)\n",
    "                    kwargs = dict(successes=self.evaluations_successes)\n",
    "\n",
    "                np.savez(\n",
    "                    self.log_path,\n",
    "                    timesteps=self.evaluations_timesteps,\n",
    "                    results=self.evaluations_results,\n",
    "                    ep_lengths=self.evaluations_length,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            if self.verbose >= 1:\n",
    "                print(f\"Eval num_timesteps={self.num_timesteps}, \" f\"episode_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "                print(f\"Episode length: {mean_ep_length:.2f} +/- {std_ep_length:.2f}\")\n",
    "            # Add to current Logger\n",
    "            self.logger.record(\"eval/mean_reward\", float(mean_reward))\n",
    "            self.logger.record(\"eval/mean_ep_length\", mean_ep_length)\n",
    "\n",
    "            if len(self._is_success_buffer) > 0:\n",
    "                success_rate = np.mean(self._is_success_buffer)\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Success rate: {100 * success_rate:.2f}%\")\n",
    "                self.logger.record(\"eval/success_rate\", success_rate)\n",
    "\n",
    "            # Dump log so the evaluation results are printed with the correct timestep\n",
    "            self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
    "            self.logger.dump(self.num_timesteps)\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                if self.verbose >= 1:\n",
    "                    print(\"New best mean reward!\")\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(os.path.join(self.best_model_save_path, \"best_model\"))\n",
    "                    self.model.save_replay_buffer(os.path.join(self.best_model_save_path, \"best_model_buffer\"))\n",
    "                    self.model.policy.save(os.path.join(self.best_model_save_path, \"best_policy\"))\n",
    "                    #self.model.get_env().env_method(\"save\", [os.path.join(self.best_model_save_path, \"best_model_env\")])      \n",
    "                    \n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Trigger callback on new best model, if needed\n",
    "                if self.callback_on_new_best is not None:\n",
    "                    continue_training = self.callback_on_new_best.on_step()\n",
    "\n",
    "            # Trigger callback after every evaluation, if needed\n",
    "            if self.callback is not None:\n",
    "                continue_training = continue_training and self._on_event()\n",
    "\n",
    "        return continue_training\n",
    "\n",
    "    def update_child_locals(self, locals_: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Update the references to the local variables.\n",
    "        :param locals_: the local variables during rollout collection\n",
    "        \"\"\"\n",
    "        if self.callback:\n",
    "            self.callback.update_locals(locals_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EvalCallbackButBetter(eval_env=env2, verbose=2, best_model_save_path=CHECKPOINT_DIR,\n",
    "                             log_path=LOG_DIR, eval_freq=2000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    #model = PPO('MultiInputPolicy', env2, tensorboard_log=LOG_DIR, learning_rate=0.005, verbose=2, policy_kwargs=dict(normalize_images=False))\n",
    "    model = DQN('CnnPolicy', env2, tensorboard_log=LOG_DIR, verbose=2, buffer_size=12000, learning_rate=0.0006, \n",
    "        learning_starts=500, exploration_fraction=0.4, exploration_initial_eps=0.9, exploration_final_eps=0.05,\n",
    "                policy_kwargs=dict(normalize_images=False))\n",
    "    model.set_random_seed(42)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/DQN_0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 150      |\n",
      "|    ep_rew_mean      | 150      |\n",
      "|    exploration_rate | 0.872    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 600      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.089    |\n",
      "|    n_updates        | 24       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 126      |\n",
      "|    ep_rew_mean      | 126      |\n",
      "|    exploration_rate | 0.852    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 19       |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 1008     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.017    |\n",
      "|    n_updates        | 126      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 116      |\n",
      "|    ep_rew_mean      | 116      |\n",
      "|    exploration_rate | 0.834    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 1396     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0216   |\n",
      "|    n_updates        | 223      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 112      |\n",
      "|    ep_rew_mean      | 112      |\n",
      "|    exploration_rate | 0.816    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 18       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 1788     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 321      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=156.40 +/- 17.73\n",
      "Episode length: 156.40 +/- 17.73\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 156      |\n",
      "|    mean_reward      | 156      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.806    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 374      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 108      |\n",
      "|    ep_rew_mean      | 108      |\n",
      "|    exploration_rate | 0.793    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 2259     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00821  |\n",
      "|    n_updates        | 439      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.776    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 181      |\n",
      "|    total_timesteps  | 2620     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 529      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.758    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 3004     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.016    |\n",
      "|    n_updates        | 625      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.739    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total_timesteps  | 3412     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0181   |\n",
      "|    n_updates        | 727      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.72     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 3805     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 826      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=144.80 +/- 9.39\n",
      "Episode length: 144.80 +/- 9.39\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 145      |\n",
      "|    mean_reward      | 145      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.711    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 874      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.699    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 310      |\n",
      "|    total_timesteps  | 4265     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 941      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.681    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 332      |\n",
      "|    total_timesteps  | 4637     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 1034     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.663    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 356      |\n",
      "|    total_timesteps  | 5020     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 1129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.643    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 381      |\n",
      "|    total_timesteps  | 5437     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0176   |\n",
      "|    n_updates        | 1234     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.623    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 14       |\n",
      "|    time_elapsed     | 408      |\n",
      "|    total_timesteps  | 5875     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 1343     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=6000, episode_reward=165.60 +/- 24.60\n",
      "Episode length: 165.60 +/- 24.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 166      |\n",
      "|    mean_reward      | 166      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.617    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 1374     |\n",
      "----------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:272: UserWarning: Path 'checkpoints\\best_model_buffer.pkl' exists, will overwrite it.\n",
      "  warnings.warn(f\"Path '{path}' exists, will overwrite it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.604    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 469      |\n",
      "|    total_timesteps  | 6269     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0204   |\n",
      "|    n_updates        | 1442     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.585    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 493      |\n",
      "|    total_timesteps  | 6668     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0227   |\n",
      "|    n_updates        | 1541     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.568    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 515      |\n",
      "|    total_timesteps  | 7033     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00677  |\n",
      "|    n_updates        | 1633     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.548    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 541      |\n",
      "|    total_timesteps  | 7457     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 1739     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.529    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 565      |\n",
      "|    total_timesteps  | 7861     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0106   |\n",
      "|    n_updates        | 1840     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=166.40 +/- 21.35\n",
      "Episode length: 166.40 +/- 21.35\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 166      |\n",
      "|    mean_reward      | 166      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.522    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 1874     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.511    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 625      |\n",
      "|    total_timesteps  | 8241     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 1935     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.492    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 649      |\n",
      "|    total_timesteps  | 8637     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 2034     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.472    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 674      |\n",
      "|    total_timesteps  | 9053     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0226   |\n",
      "|    n_updates        | 2138     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 700      |\n",
      "|    total_timesteps  | 9477     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0197   |\n",
      "|    n_updates        | 2244     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.435    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 723      |\n",
      "|    total_timesteps  | 9857     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 2339     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=152.80 +/- 30.64\n",
      "Episode length: 152.80 +/- 30.64\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 153      |\n",
      "|    mean_reward      | 153      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.428    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0133   |\n",
      "|    n_updates        | 2374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.413    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 779      |\n",
      "|    total_timesteps  | 10304    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0245   |\n",
      "|    n_updates        | 2450     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.6     |\n",
      "|    ep_rew_mean      | 99.6     |\n",
      "|    exploration_rate | 0.388    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 811      |\n",
      "|    total_timesteps  | 10833    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 2583     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.365    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 841      |\n",
      "|    total_timesteps  | 11325    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0208   |\n",
      "|    n_updates        | 2706     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.345    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 866      |\n",
      "|    total_timesteps  | 11752    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0088   |\n",
      "|    n_updates        | 2812     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=12000, episode_reward=149.00 +/- 9.21\n",
      "Episode length: 149.00 +/- 9.21\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 149      |\n",
      "|    mean_reward      | 149      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.333    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0256   |\n",
      "|    n_updates        | 2874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.324    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 922      |\n",
      "|    total_timesteps  | 12208    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 2926     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.299    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 952      |\n",
      "|    total_timesteps  | 12720    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00994  |\n",
      "|    n_updates        | 3054     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.281    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 975      |\n",
      "|    total_timesteps  | 13098    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0336   |\n",
      "|    n_updates        | 3149     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.263    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 999      |\n",
      "|    total_timesteps  | 13499    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0101   |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.242    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1024     |\n",
      "|    total_timesteps  | 13929    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0175   |\n",
      "|    n_updates        | 3357     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=145.40 +/- 33.56\n",
      "Episode length: 145.40 +/- 33.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 145      |\n",
      "|    mean_reward      | 145      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.239    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0071   |\n",
      "|    n_updates        | 3374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.219    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1083     |\n",
      "|    total_timesteps  | 14421    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0237   |\n",
      "|    n_updates        | 3480     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.198    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1110     |\n",
      "|    total_timesteps  | 14865    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 3591     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.18     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1133     |\n",
      "|    total_timesteps  | 15241    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0076   |\n",
      "|    n_updates        | 3685     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.162    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1155     |\n",
      "|    total_timesteps  | 15624    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 3780     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=194.00 +/- 24.95\n",
      "Episode length: 194.00 +/- 24.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 194      |\n",
      "|    mean_reward      | 194      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.144    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0374   |\n",
      "|    n_updates        | 3874     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.143    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1217     |\n",
      "|    total_timesteps  | 16041    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 3885     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.124    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1241     |\n",
      "|    total_timesteps  | 16442    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0238   |\n",
      "|    n_updates        | 3985     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.102    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1268     |\n",
      "|    total_timesteps  | 16893    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 4098     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.0855   |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1290     |\n",
      "|    total_timesteps  | 17248    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0195   |\n",
      "|    n_updates        | 4186     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.0675   |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1312     |\n",
      "|    total_timesteps  | 17629    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0212   |\n",
      "|    n_updates        | 4282     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=156.60 +/- 20.76\n",
      "Episode length: 156.60 +/- 20.76\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 157      |\n",
      "|    mean_reward      | 157      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00876  |\n",
      "|    n_updates        | 4374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1371     |\n",
      "|    total_timesteps  | 18089    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 4397     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1395     |\n",
      "|    total_timesteps  | 18486    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00463  |\n",
      "|    n_updates        | 4496     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1422     |\n",
      "|    total_timesteps  | 18922    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0089   |\n",
      "|    n_updates        | 4605     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 104      |\n",
      "|    ep_rew_mean      | 104      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1444     |\n",
      "|    total_timesteps  | 19295    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 4698     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1476     |\n",
      "|    total_timesteps  | 19830    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 4832     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=153.60 +/- 20.15\n",
      "Episode length: 153.60 +/- 20.15\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 154      |\n",
      "|    mean_reward      | 154      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 4874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1532     |\n",
      "|    total_timesteps  | 20261    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 4940     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1556     |\n",
      "|    total_timesteps  | 20647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0183   |\n",
      "|    n_updates        | 5036     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 105      |\n",
      "|    ep_rew_mean      | 105      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1579     |\n",
      "|    total_timesteps  | 21021    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0474   |\n",
      "|    n_updates        | 5130     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1603     |\n",
      "|    total_timesteps  | 21413    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00826  |\n",
      "|    n_updates        | 5228     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1627     |\n",
      "|    total_timesteps  | 21801    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0188   |\n",
      "|    n_updates        | 5325     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=148.80 +/- 18.24\n",
      "Episode length: 148.80 +/- 18.24\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 149      |\n",
      "|    mean_reward      | 149      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 5374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1688     |\n",
      "|    total_timesteps  | 22305    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0421   |\n",
      "|    n_updates        | 5451     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1710     |\n",
      "|    total_timesteps  | 22678    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 5544     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1735     |\n",
      "|    total_timesteps  | 23080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 5644     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1758     |\n",
      "|    total_timesteps  | 23445    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0362   |\n",
      "|    n_updates        | 5736     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.8     |\n",
      "|    ep_rew_mean      | 99.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1781     |\n",
      "|    total_timesteps  | 23817    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00804  |\n",
      "|    n_updates        | 5829     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=165.60 +/- 46.85\n",
      "Episode length: 165.60 +/- 46.85\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 166      |\n",
      "|    mean_reward      | 166      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00728  |\n",
      "|    n_updates        | 5874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.3     |\n",
      "|    ep_rew_mean      | 99.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1840     |\n",
      "|    total_timesteps  | 24201    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00691  |\n",
      "|    n_updates        | 5925     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.2     |\n",
      "|    ep_rew_mean      | 99.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1865     |\n",
      "|    total_timesteps  | 24613    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00691  |\n",
      "|    n_updates        | 6028     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.6     |\n",
      "|    ep_rew_mean      | 98.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1889     |\n",
      "|    total_timesteps  | 24999    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00766  |\n",
      "|    n_updates        | 6124     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.6     |\n",
      "|    ep_rew_mean      | 98.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1911     |\n",
      "|    total_timesteps  | 25373    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00743  |\n",
      "|    n_updates        | 6218     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.9     |\n",
      "|    ep_rew_mean      | 98.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1936     |\n",
      "|    total_timesteps  | 25787    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0262   |\n",
      "|    n_updates        | 6321     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=149.00 +/- 26.02\n",
      "Episode length: 149.00 +/- 26.02\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 149      |\n",
      "|    mean_reward      | 149      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 26000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 6374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.8     |\n",
      "|    ep_rew_mean      | 98.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 1990     |\n",
      "|    total_timesteps  | 26176    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00985  |\n",
      "|    n_updates        | 6418     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99       |\n",
      "|    ep_rew_mean      | 99       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2016     |\n",
      "|    total_timesteps  | 26593    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.029    |\n",
      "|    n_updates        | 6523     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.4     |\n",
      "|    ep_rew_mean      | 98.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2039     |\n",
      "|    total_timesteps  | 26982    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00751  |\n",
      "|    n_updates        | 6620     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.1     |\n",
      "|    ep_rew_mean      | 99.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2066     |\n",
      "|    total_timesteps  | 27410    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00984  |\n",
      "|    n_updates        | 6727     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.9     |\n",
      "|    ep_rew_mean      | 99.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2094     |\n",
      "|    total_timesteps  | 27866    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00379  |\n",
      "|    n_updates        | 6841     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=128.80 +/- 17.71\n",
      "Episode length: 128.80 +/- 17.71\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 129      |\n",
      "|    mean_reward      | 129      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00708  |\n",
      "|    n_updates        | 6874     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2152     |\n",
      "|    total_timesteps  | 28342    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00393  |\n",
      "|    n_updates        | 6960     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2177     |\n",
      "|    total_timesteps  | 28756    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00804  |\n",
      "|    n_updates        | 7063     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.5     |\n",
      "|    ep_rew_mean      | 99.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2200     |\n",
      "|    total_timesteps  | 29125    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00424  |\n",
      "|    n_updates        | 7156     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.7     |\n",
      "|    ep_rew_mean      | 99.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2224     |\n",
      "|    total_timesteps  | 29518    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00947  |\n",
      "|    n_updates        | 7254     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.2     |\n",
      "|    ep_rew_mean      | 98.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2248     |\n",
      "|    total_timesteps  | 29897    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0434   |\n",
      "|    n_updates        | 7349     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=131.60 +/- 10.89\n",
      "Episode length: 131.60 +/- 10.89\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 132      |\n",
      "|    mean_reward      | 132      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00539  |\n",
      "|    n_updates        | 7374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.7     |\n",
      "|    ep_rew_mean      | 98.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2302     |\n",
      "|    total_timesteps  | 30321    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0716   |\n",
      "|    n_updates        | 7455     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.4     |\n",
      "|    ep_rew_mean      | 98.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2324     |\n",
      "|    total_timesteps  | 30677    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 7544     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.2     |\n",
      "|    ep_rew_mean      | 99.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2352     |\n",
      "|    total_timesteps  | 31137    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0606   |\n",
      "|    n_updates        | 7659     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.3     |\n",
      "|    ep_rew_mean      | 99.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2376     |\n",
      "|    total_timesteps  | 31534    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00662  |\n",
      "|    n_updates        | 7758     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.9     |\n",
      "|    ep_rew_mean      | 98.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2398     |\n",
      "|    total_timesteps  | 31887    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 7846     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=222.20 +/- 24.21\n",
      "Episode length: 222.20 +/- 24.21\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 222      |\n",
      "|    mean_reward      | 222      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0798   |\n",
      "|    n_updates        | 7874     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.3     |\n",
      "|    ep_rew_mean      | 98.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2459     |\n",
      "|    total_timesteps  | 32233    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 7933     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.4     |\n",
      "|    ep_rew_mean      | 98.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2482     |\n",
      "|    total_timesteps  | 32613    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.014    |\n",
      "|    n_updates        | 8028     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.5     |\n",
      "|    ep_rew_mean      | 98.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2508     |\n",
      "|    total_timesteps  | 33030    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 8132     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.6     |\n",
      "|    ep_rew_mean      | 98.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2531     |\n",
      "|    total_timesteps  | 33407    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0226   |\n",
      "|    n_updates        | 8226     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99       |\n",
      "|    ep_rew_mean      | 99       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2555     |\n",
      "|    total_timesteps  | 33820    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0622   |\n",
      "|    n_updates        | 8329     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=133.00 +/- 24.49\n",
      "Episode length: 133.00 +/- 24.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 133      |\n",
      "|    mean_reward      | 133      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0382   |\n",
      "|    n_updates        | 8374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.6     |\n",
      "|    ep_rew_mean      | 99.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2617     |\n",
      "|    total_timesteps  | 34340    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00622  |\n",
      "|    n_updates        | 8459     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2646     |\n",
      "|    total_timesteps  | 34804    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.035    |\n",
      "|    n_updates        | 8575     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2670     |\n",
      "|    total_timesteps  | 35205    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0495   |\n",
      "|    n_updates        | 8676     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2694     |\n",
      "|    total_timesteps  | 35585    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00518  |\n",
      "|    n_updates        | 8771     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.7     |\n",
      "|    ep_rew_mean      | 99.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2715     |\n",
      "|    total_timesteps  | 35939    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00834  |\n",
      "|    n_updates        | 8859     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=171.20 +/- 32.54\n",
      "Episode length: 171.20 +/- 32.54\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 171      |\n",
      "|    mean_reward      | 171      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0077   |\n",
      "|    n_updates        | 8874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.7     |\n",
      "|    ep_rew_mean      | 99.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2772     |\n",
      "|    total_timesteps  | 36369    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00417  |\n",
      "|    n_updates        | 8967     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.5     |\n",
      "|    ep_rew_mean      | 99.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2795     |\n",
      "|    total_timesteps  | 36757    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 9064     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.8     |\n",
      "|    ep_rew_mean      | 99.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2821     |\n",
      "|    total_timesteps  | 37185    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00617  |\n",
      "|    n_updates        | 9171     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2849     |\n",
      "|    total_timesteps  | 37651    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00425  |\n",
      "|    n_updates        | 9287     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=160.40 +/- 48.12\n",
      "Episode length: 160.40 +/- 48.12\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 160      |\n",
      "|    mean_reward      | 160      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 9374     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.5     |\n",
      "|    ep_rew_mean      | 99.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2909     |\n",
      "|    total_timesteps  | 38089    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00935  |\n",
      "|    n_updates        | 9397     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 99.5     |\n",
      "|    ep_rew_mean      | 99.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2936     |\n",
      "|    total_timesteps  | 38530    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00549  |\n",
      "|    n_updates        | 9507     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2963     |\n",
      "|    total_timesteps  | 38989    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 9622     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 2988     |\n",
      "|    total_timesteps  | 39401    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 9725     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3011     |\n",
      "|    total_timesteps  | 39786    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00482  |\n",
      "|    n_updates        | 9821     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=168.80 +/- 11.55\n",
      "Episode length: 168.80 +/- 11.55\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 169      |\n",
      "|    mean_reward      | 169      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 9874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3066     |\n",
      "|    total_timesteps  | 40213    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0299   |\n",
      "|    n_updates        | 9928     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3092     |\n",
      "|    total_timesteps  | 40641    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0318   |\n",
      "|    n_updates        | 10035    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3118     |\n",
      "|    total_timesteps  | 41074    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 10143    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3142     |\n",
      "|    total_timesteps  | 41453    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0096   |\n",
      "|    n_updates        | 10238    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 100      |\n",
      "|    ep_rew_mean      | 100      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3164     |\n",
      "|    total_timesteps  | 41825    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0177   |\n",
      "|    n_updates        | 10331    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=114.80 +/- 5.71\n",
      "Episode length: 114.80 +/- 5.71\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 115      |\n",
      "|    mean_reward      | 115      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 10374    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 101      |\n",
      "|    ep_rew_mean      | 101      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3221     |\n",
      "|    total_timesteps  | 42305    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 10451    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3246     |\n",
      "|    total_timesteps  | 42729    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00846  |\n",
      "|    n_updates        | 10557    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 103      |\n",
      "|    ep_rew_mean      | 103      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3273     |\n",
      "|    total_timesteps  | 43182    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0184   |\n",
      "|    n_updates        | 10670    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3296     |\n",
      "|    total_timesteps  | 43553    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.04     |\n",
      "|    n_updates        | 10763    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3318     |\n",
      "|    total_timesteps  | 43925    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00822  |\n",
      "|    n_updates        | 10856    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=201.60 +/- 21.77\n",
      "Episode length: 201.60 +/- 21.77\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 202      |\n",
      "|    mean_reward      | 202      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 44000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.0176   |\n",
      "|    n_updates        | 10874    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3378     |\n",
      "|    total_timesteps  | 44361    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00743  |\n",
      "|    n_updates        | 10965    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 102      |\n",
      "|    ep_rew_mean      | 102      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3406     |\n",
      "|    total_timesteps  | 44819    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0006   |\n",
      "|    loss             | 0.00517  |\n",
      "|    n_updates        | 11079    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x18fc2fa8ac0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Começa o treinamento\n",
    "model._last_obs = None\n",
    "model.learn(total_timesteps=45000, callback=callback, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização de hiper parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biblioteca para isso\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criandos as pastas para que funcione.\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para tentar retornar os hiper parametros de teste e definir a função objetivo\n",
    "#ALTERAR:\n",
    "#Essa função deve ser modificada com os respectivos Hiper-Parametros desejados para teste\n",
    "def otimizar_modelo(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048,8192),\n",
    "        'gamma': trial.suggest_float('gamma', 0.8,0.999),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5,1e-4),\n",
    "        'clip_range': trial.suggest_float('clip_range', 0.1,0.4),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.8,0.99),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para testar hiper parametros\n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = otimizar_modelo(trial)\n",
    "        #ALTERAR:\n",
    "        # Colocar a instanciação do eventos correta.\n",
    "        env_lambda = lambda: Monitor(WebGame(), LOG_DIR)\n",
    "        env2 = SubprocVecEnv([env_lambda for i in range(4)])\n",
    "        # REVISAR:\n",
    "        # Pq eu apenas queria colocar o mean_reward no log, visto que com a função criada a gente já salva o reward com a melhor performance.\n",
    "        #Isso sou tentando fazer plotar as paradas pro tensorboard. Eu não queria salvar os modelos, pq o optuna ja faz isso.\n",
    "        checkpoint_callback = CheckpointCallback(\n",
    "            save_freq=500,\n",
    "            save_path=CHECKPOINT_DIR,\n",
    "            name_prefix=\"model_ppo_opt\", #ALTERAR\n",
    "            save_replay_buffer=True,\n",
    "            save_vecnormalize=True,\n",
    "            verbose=2\n",
    "        )\n",
    "        eval_callback = EvalCallback(env2,\n",
    "                             log_path=LOG_DIR, eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "        callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "        \n",
    "        #ALTERAR:\n",
    "        #Alterar o modelo para oq precisa. A única coisa que deve se manter é o '**model_params' pois isso é responsavel para passar para os modelos os hiper parametros necessários para otimização.\n",
    "        model = PPO('CnnPolicy', env2, tensorboard_log=LOG_DIR, verbose=2,device='cpu', **model_params, policy_kwargs=dict(normalize_images=False))\n",
    "        \n",
    "        model.learn(total_timesteps=160000)\n",
    "        \n",
    "        mean_reward,_ = evaluate_policy(model,env2,n_eval_episodes=5)\n",
    "        env2.close()\n",
    "        # ALTERAR\n",
    "        # Eu não sei se vai salvar o buffer, precisaria corrigir. Mas tamo ai\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "#Parametros:\n",
    "#Função a ser otimizada,\n",
    "#Número de tentativas de teste,\n",
    "#N_jobs: número de processos em paralelos,\n",
    "#gc_after_trials: garbage collector para que meu pc n exploda sem memória.\n",
    "study.optimize(optimize_agent, n_trials=10, n_jobs=1, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_8856\\1390173097.py:26: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self._driver = webdriver.Chrome(executable_path=self.chromedriver_path,options=_chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "mean_reward: 242.7\n",
      "std_reward: 57.711437341310436\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.dqn.policies import CnnPolicy\n",
    "\n",
    "env2 = WebGame()\n",
    "model = get_model()\n",
    "model.load('checkpoints/best_model') \n",
    "model.load_replay_buffer('checkpoints/best_model_buffer') \n",
    "saved_policy = CnnPolicy.load(\"checkpoints/best_policy\")\n",
    "\n",
    "# Evaluate the loaded policy\n",
    "mean_reward, std_reward = evaluate_policy(saved_policy, env2, n_eval_episodes=10, deterministic=True)\n",
    "print('mean_reward: {}'.format(mean_reward))\n",
    "print('std_reward: {}'.format(std_reward))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12348), started 0:24:12 ago. (Use '!kill 12348' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs/DQN_0/ --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9105c753ba2e810590fbe03dc7a47b222fde04124da1bb9f952ac899bb56b210"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
